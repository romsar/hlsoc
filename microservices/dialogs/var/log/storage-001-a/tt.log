Watchdog 2024/06/04 01:03:35 (INFO): using "/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/config.yaml" cluster config for instance "storage-001-a"
started
2024-06-04 01:03:35.265 [68363] main/104/interactive I> Tarantool 3.1.0-0-g96f6d88 Darwin-x86_64-Release
2024-06-04 01:03:35.266 [68363] main/104/interactive I> log level 5 (INFO)
2024-06-04 01:03:35.266 [68363] main/104/interactive I> wal/engine cleanup is paused
2024-06-04 01:03:35.266 [68363] main/104/interactive I> mapping 268435456 bytes for memtx tuple arena...
2024-06-04 01:03:35.266 [68363] main/104/interactive I> Actual slab_alloc_factor calculated on the basis of desired slab_alloc_factor = 1.044274
2024-06-04 01:03:35.266 [68363] main/104/interactive I> mapping 134217728 bytes for vinyl tuple arena...
2024-06-04 01:03:35.267 [68363] main/104/interactive/box.upgrade I> Recovering snapshot with schema version 3.1.0
2024-06-04 01:03:35.267 [68363] main/104/interactive/tarantool.config log.lua:130 W> box_cfg.apply: instance storage-001-b is unknown. Possibly instance_name is not set in database and UUID is not specified. Or instance have not joined yet.
2024-06-04 01:03:35.268 [68363] main/104/interactive I> update replication_synchro_quorum = 1
2024-06-04 01:03:35.268 [68363] main/104/interactive I> instance uuid 33425648-d2aa-4911-9c90-dd6ec549d048
2024-06-04 01:03:35.269 [68363] main/104/interactive I> instance vclock {1: 1069}
2024-06-04 01:03:35.270 [68363] main/104/interactive I> tx_binary: bound to [::1]:3301
2024-06-04 01:03:35.270 [68363] main/104/interactive I> connecting to 2 replicas
2024-06-04 01:03:35.271 [68363] main/114/applier/replicator@localhost:3302 I> remote master 7411d451-49e1-4373-9fed-0a2c2a7b10a8 at [::1]:3302 running Tarantool 3.1.0
2024-06-04 01:03:35.271 [68363] main/113/applier/replicator@localhost:3301 I> remote master 33425648-d2aa-4911-9c90-dd6ec549d048 at [::1]:3301 running Tarantool 3.1.0
2024-06-04 01:03:35.273 [68363] main/104/interactive I> connected to 2 replicas
2024-06-04 01:03:35.273 [68363] main/104/interactive I> recovery start
2024-06-04 01:03:35.273 [68363] main/104/interactive I> recovering from `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000000000.snap'
2024-06-04 01:03:35.273 [68363] main/104/interactive I> replicaset name: storage-001
2024-06-04 01:03:35.273 [68363] main/104/interactive I> replicaset uuid c903494a-bc47-40f2-9266-58472b3f5734
2024-06-04 01:03:35.290 [68363] main/104/interactive I> assigned id 1 to replica 33425648-d2aa-4911-9c90-dd6ec549d048
2024-06-04 01:03:35.290 [68363] main/104/interactive I> update replication_synchro_quorum = 1
2024-06-04 01:03:35.290 [68363] main/104/interactive I> assigned name storage-001-a to replica 33425648-d2aa-4911-9c90-dd6ec549d048
2024-06-04 01:03:35.290 [68363] main/104/interactive I> recover from `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000000000.xlog'
2024-06-04 01:03:35.291 [68363] main/104/interactive I> assigned id 2 to replica 7411d451-49e1-4373-9fed-0a2c2a7b10a8
2024-06-04 01:03:35.291 [68363] main/104/interactive I> update replication_synchro_quorum = 2
2024-06-04 01:03:35.291 [68363] main/104/interactive I> RAFT: fencing paused
2024-06-04 01:03:35.291 [68363] main/104/interactive I> assigned name storage-001-b to replica 7411d451-49e1-4373-9fed-0a2c2a7b10a8
2024-06-04 01:03:35.291 [68363] main/104/interactive I> done `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000000000.xlog'
2024-06-04 01:03:35.291 [68363] main/104/interactive I> recover from `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001066.xlog'
2024-06-04 01:03:35.291 [68363] main/104/interactive I> done `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001066.xlog'
2024-06-04 01:03:35.291 [68363] main/104/interactive I> recover from `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001068.xlog'
2024-06-04 01:03:35.291 [68363] main/104/interactive I> done `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001068.xlog'
2024-06-04 01:03:35.292 [68363] main/104/interactive I> recover from `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001069.xlog'
2024-06-04 01:03:35.292 [68363] main/104/interactive I> done `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001069.xlog'
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Building secondary indexes in space '_bucket'...
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Adding 500 keys to TREE index 'status' ...
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Space '_bucket': done
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Building secondary indexes in space 'messages'...
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Adding 5 keys to TREE index 'bucket_id' ...
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Adding 5 keys to TREE index 'dialog_hash' ...
2024-06-04 01:03:35.292 [68363] main/104/interactive I> Space 'messages': done
2024-06-04 01:03:35.292 [68363] main/104/interactive I> ready to accept requests
2024-06-04 01:03:35.292 [68363] main/104/interactive I> synchronizing with 2 replicas
2024-06-04 01:03:35.292 [68363] main/104/interactive C> failed to synchronize with 1 out of 2 replicas
2024-06-04 01:03:35.292 [68363] main/104/interactive I> entering orphan mode
2024-06-04 01:03:35.292 [68363] main/104/interactive/box.load_cfg I> set 'instance_name' configuration option to "storage-001-a"
2024-06-04 01:03:35.292 [68363] main/104/interactive/box.load_cfg I> set 'custom_proc_title' configuration option to "tarantool - storage-001-a"
2024-06-04 01:03:35.292 [68363] main/104/interactive/box.load_cfg I> set 'log_nonblock' configuration option to false
2024-06-04 01:03:35.292 [68363] main/104/interactive/box.load_cfg I> set 'replication' configuration option to ["replicator@localhost:3301","replicator@localhost:3302"]
2024-06-04 01:03:35.292 [68363] main/104/interactive/box.load_cfg I> set 'replicaset_name' configuration option to "storage-001"
2024-06-04 01:03:35.293 [68363] main/104/interactive/box.load_cfg I> set 'listen' configuration option to [{"uri":"localhost:3301"}]
2024-06-04 01:03:35.293 [68363] main/107/checkpoint_daemon I> scheduled next checkpoint for Tue Jun  4 02:26:30 2024
2024-06-04 01:03:35.293 [68363] main/104/interactive/box.load_cfg I> set 'metrics' configuration option to {"labels":{"alias":"storage-001-a"},"include":["all"],"exclude":[]}
2024-06-04 01:03:35.293 [68363] main/104/interactive/box.load_cfg I> set 'read_only' configuration option to true
2024-06-04 01:03:35.293 [68363] main/114/applier/replicator@localhost:3302 I> authenticated
2024-06-04 01:03:35.293 [68363] main/112/main I> subscribed replica 7411d451-49e1-4373-9fed-0a2c2a7b10a8 at fd 29, aka [::1]:3301, peer of [::1]:59653
2024-06-04 01:03:35.293 [68363] main/112/main I> remote vclock {1: 1069} local vclock {1: 1069}
2024-06-04 01:03:35.293 [68363] main/106/gc I> wal/engine cleanup is resumed
2024-06-04 01:03:35.294 [68363] relay/[::1]:59653/101/main I> recover from `/Users/r.sarvarov/projects/hlsoc/microservices/dialogs/var/lib/storage-001-a/00000000000000001069.xlog'
2024-06-04 01:03:35.308 [68363] main/114/applier/replicator@localhost:3302 I> subscribed
2024-06-04 01:03:35.308 [68363] main/114/applier/replicator@localhost:3302 I> remote vclock {1: 1069} local vclock {1: 1069}
2024-06-04 01:03:35.308 [68363] main/114/applier/replicator@localhost:3302 I> RAFT: message {term: 1, state: follower} from 2
2024-06-04 01:03:35.308 [68363] main/114/applier/replicator@localhost:3302 I> leaving orphan mode
2024-06-04 01:03:35.310 [68363] main/123/console/unix/: sio.c:315 !> getpeername: Invalid argument
2024-06-04 01:03:35.310 [68363] main/104/interactive I> update replication_synchro_quorum = 2
2024-06-04 01:03:35.311 [68363] main/104/interactive/box.load_cfg I> set 'read_only' configuration option to false
2024-06-04 01:03:35.311 [68363] main/107/checkpoint_daemon I> scheduled next checkpoint for Tue Jun  4 02:30:39 2024
2024-06-04 01:03:35.348 [68363] main/104/interactive/tarantool.config I> sharding: apply storage config
2024-06-04 01:03:35.348 [68363] main/104/interactive/vshard.storage I> Starting configuration of replica storage-001-a
2024-06-04 01:03:35.348 [68363] main/104/interactive/vshard.storage I> Box configuration was skipped due to the 'manual' box_cfg_mode
2024-06-04 01:03:35.348 [68363] main/104/interactive/vshard.storage I> Stepping up into the master role
2024-06-04 01:03:35.348 [68363] main/129/box.watcher/vshard.storage.exports I> Deploying exports for vshard 0.1.16.0, core 3.1.0-0
2024-06-04 01:03:35.348 [68363] main/128/vshard.state_watch/vshard.util I> instance_watch_f has been started
2024-06-04 01:03:35.348 [68363] main/130/vshard.conn_man/vshard.util I> conn_manager_f has been started
2024-06-04 01:03:35.348 [68363] main/131/vshard.gc/vshard.util I> gc_bucket_f has been started
2024-06-04 01:03:35.348 [68363] main/132/vshard.recovery/vshard.util I> recovery_f has been started
2024-06-04 01:03:35.349 [68363] main/104/interactive/vshard.storage I> Starting the rebalancer
2024-06-04 01:03:35.349 [68363] main/133/vshard.rebalancer/vshard.util I> rebalancer_f has been started
2024-06-04 01:03:35.356 [68363] main I> entering the event loop
2024-06-04 01:03:35.358 [68363] main/134/localhost:3302 (net.box)/vshard.replicaset I> connected to localhost:3302
2024-06-04 01:03:35.358 [68363] main/135/localhost:3301 (net.box)/vshard.replicaset I> connected to localhost:3301
2024-06-04 01:03:35.849 [68363] main/130/vshard.conn_man/vshard.replicaset I> Found master for replicaset storage-001: storage-001-a
2024-06-04 01:03:35.853 [68363] main/138/localhost:3304 (net.box)/vshard.replicaset I> connected to localhost:3304
2024-06-04 01:03:35.855 [68363] main/139/localhost:3303 (net.box)/vshard.replicaset I> connected to localhost:3303
2024-06-04 01:03:36.351 [68363] main/130/vshard.conn_man/vshard.replicaset I> Found master for replicaset storage-002: storage-002-a
2024-06-04 01:03:36.352 [68363] main/133/vshard.rebalancer/vshard.storage I> The cluster is balanced ok. Schedule next rebalancing after 3600.000000 seconds
